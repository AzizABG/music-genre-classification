{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary libraries\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import scipy.io.wavfile as wav\n",
    "from python_speech_features import mfcc\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "# RandomizedSearchCV\n",
    "# GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessAudioFiles(directory, outputFile):\n",
    "    \"\"\"\n",
    "    Preprocess audio files to extract MFCC features and save to a file.\n",
    "    This function iterates through all the files in the specified directory,\n",
    "    extracts their MFCC features, and saves these features along with covariance\n",
    "    and mean matrix for each file.\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    with open(outputFile, 'wb') as f:\n",
    "        for i, folder in enumerate(filter(lambda f: f != \".DS_Store\", os.listdir(directory)), start=1):\n",
    "            if i == 11:\n",
    "                break\n",
    "            for file in filter(lambda f: f != \".DS_Store\", os.listdir(os.path.join(directory, folder))):\n",
    "\n",
    "                # Read the WAV file.\n",
    "                rate, sig = wav.read(os.path.join(directory, folder, file))\n",
    "                # Calculate the number of samples per 10-second chunk.\n",
    "                samples_per_chunk = 10 * rate\n",
    "                for j in range(0, len(sig), samples_per_chunk):\n",
    "                    # Get the current chunk of audio.\n",
    "                    chunk = sig[j:j + samples_per_chunk]\n",
    "                    if len(chunk) == samples_per_chunk:\n",
    "                            # Compute MFCC features from the audio chunk.\n",
    "                            mfcc_feat = mfcc(chunk, rate, winlen=0.025, appendEnergy=False, nfft=1200)\n",
    "                            # Skip covariance calculation if there are not enough frames.\n",
    "                            # if mfcc_feat.shape[0] > mfcc_feat.shape[1]:\n",
    "                            # Calculate the covariance matrix of the MFCC features.\n",
    "                            covariance = np.cov(mfcc_feat.T)\n",
    "                            inverse_covariance = np.linalg.inv(covariance)\n",
    "                            # Calculate the mean of the MFCC features.\n",
    "                            mean_vector = mfcc_feat.mean(axis=0)\n",
    "                            # Store the features and the genre label.\n",
    "                            feature = (mean_vector, covariance, inverse_covariance, i)\n",
    "                            print(f'The folder: {i}')\n",
    "                            pickle.dump(feature, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(filename):\n",
    "    \"\"\"\n",
    "    Load and process a dataset from a pickle file.\n",
    "    Each entry in the file is a tuple containing a mean vector, a covariance matrix,\n",
    "    and a genre identifier. This function combines these elements into a format\n",
    "    suitable for use with machine learning models.\n",
    "    \"\"\"\n",
    "\n",
    "    mean_data, cov_data, cov_inv_data, genre_data= [], [], [], []\n",
    "\n",
    "    # Load data from the file\n",
    "    with open(filename, 'rb') as f:\n",
    "        while True:\n",
    "            try:\n",
    "                mean, cov, cov_inv, genre = pickle.load(f)\n",
    "            except EOFError:\n",
    "                break\n",
    "            cov_data.append(cov.flatten())\n",
    "            mean_data.append(mean)\n",
    "            genre_data.append(genre)\n",
    "            cov_inv_data.append(cov_inv.flatten())\n",
    "            \n",
    "            \n",
    "     \n",
    "     \n",
    "    # Convert the lists to NumPy arrays and combine covariance data, their inverse, and mean data.\n",
    "    flattened_cov_data = np.array(cov_data)\n",
    "    flattened_cov_inv_data = np.array(cov_inv_data)\n",
    "    mean_data_array = np.array(mean_data)\n",
    "    \n",
    "    combined_data = np.column_stack([flattened_cov_data, flattened_cov_inv_data, mean_data_array])\n",
    "\n",
    "    return combined_data, np.array(genre_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_modified_mahalanobis_distance(X1_flat,X2_flat, n_features=13):\n",
    "    \"\"\"\n",
    "    Compute a modified Mahalanobis distance between two instances.\n",
    "    This is a custom distance function used for KNN classification.\n",
    "    \"\"\"\n",
    "\n",
    "        \n",
    "    # Reshape the first part of the instances into covariance matrices.\n",
    "    cov1, cov2 = X1_flat[:n_features**2].reshape(-1, n_features), X2_flat[:n_features**2].reshape(-1, n_features)\n",
    "        # Reshape the first part of the instances into the inverses of the covariance matrices.\n",
    "    inv_cov1, inv_cov2 = X1_flat[n_features**2:2*n_features**2].reshape(-1, n_features), X2_flat[n_features**2:2*n_features**2].reshape(-1, n_features)\n",
    "    # Extract the mean vectors from the instances.\n",
    "    mean1, mean2 = X1_flat[-n_features:], X2_flat[-n_features:]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # Compute the distance using the modified Mahalanobis formula\n",
    "    trace_term = np.trace(np.dot(inv_cov2, cov1)) + np.trace(np.dot(inv_cov1, cov2))\n",
    "    quadratic_term2 = np.dot((mean1 - mean2).T, np.dot(inv_cov1, mean1 - mean2))\n",
    "    quadratic_term1 = np.dot((mean2 - mean1).T, np.dot(inv_cov2, mean2 - mean1))\n",
    "    # log_det_term = np.log(np.linalg.det(cov2)) - np.log(np.linalg.det(cov1)) + np.log(np.linalg.det(cov1)) - np.log(np.linalg.det(cov2))\n",
    "    \n",
    "    dist = trace_term + quadratic_term1 + quadratic_term2\n",
    "    # + log_det_term \n",
    "\n",
    "    return dist\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Use RandomizedSearchCV or GridSearchCV to do hyper parameter tuning\n",
    "\n",
    "# def main():\n",
    "#     # Directory containing the audio files and output file name\n",
    "#     directory = \"./Data/genres_original/\"\n",
    "#     output_file = \"my_mfcc_Mahalanobis.dat\"\n",
    "\n",
    "#     # Preprocess the audio files and extract features.\n",
    "#     preprocessAudioFiles(directory, output_file)\n",
    "    \n",
    "#     # Load the dataset from the output file\n",
    "#     cov_mean_data, genre_data = load_dataset(output_file)\n",
    "    \n",
    "\n",
    "    \n",
    "#     # Define parameters for grid search\n",
    "#     # param_grid = {'n_neighbors': range(1, 7),\n",
    "#     #               'metric': [compute_modified_mahalanobis_distance]\n",
    "#     #               }\n",
    "#     param_dist = {'n_neighbors': range(1, 7),\n",
    "#                   }\n",
    "    \n",
    "#     # knn = KNeighborsClassifier()\n",
    "#     knn = KNeighborsClassifier(metric=compute_modified_mahalanobis_distance)\n",
    "    \n",
    "#     # grid_search = GridSearchCV(estimator=knn, param_grid=param_grid, cv=5, scoring='accuracy')\n",
    "#     random_search = RandomizedSearchCV(estimator=knn, param_distributions=param_dist, n_iter=100, cv=5, scoring='accuracy', random_state=43)\n",
    "\n",
    "#     # Perform grid search to find the best KNN parameters\n",
    "#     # grid_search.fit(cov_mean_data, genre_data)\n",
    "#     random_search.fit(cov_mean_data, genre_data)\n",
    "    \n",
    "#     # Get the best parameters and model from the grid search\n",
    "#     # best_k = grid_search.best_params_['n_neighbors']\n",
    "#     # best_model = grid_search.best_estimator_\n",
    "#     best_k = random_search.best_params_['n_neighbors']\n",
    "#     best_model = random_search.best_estimator_\n",
    "\n",
    "#     # Split the dataset into training and testing sets\n",
    "#     train_X, test_X, train_y, test_y = train_test_split(cov_mean_data, genre_data, test_size=0.2, random_state=130)\n",
    "\n",
    "#     # Train the best model on the training data\n",
    "#     best_model.fit(train_X, train_y)\n",
    "\n",
    "#     # Evaluate the model on the test data\n",
    "#     accuracy = best_model.score(test_X, test_y)\n",
    "\n",
    "#     # Print the results\n",
    "#     print(f'Best K: {best_k}')\n",
    "#     print(f'Test Accuracy: {accuracy}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do not use RandomizedSearchCV or GridSearchCV\n",
    "\n",
    "def main():\n",
    "    # Directory containing the audio files and output file name\n",
    "    directory = \"./Data/genres_original/\"\n",
    "    output_file = \"my_mfcc_Mahalanobis_chopped_inverse.dat\"\n",
    "\n",
    "\n",
    "    # preprocessAudioFiles(directory, output_file)\n",
    "    # print(\"Finished preprocessing.\")\n",
    "    \n",
    "    # Check if the output file already exists\n",
    "    if not os.path.exists(output_file):\n",
    "        # If the file does not exist, preprocess the audio files to extract features\n",
    "        print(\"Preprocessing audio files and extracting features...\")\n",
    "        preprocessAudioFiles(directory, output_file)\n",
    "        \n",
    "    else:\n",
    "        # If the file exists, skip preprocessing and use the existing file\n",
    "        print(\"Found existing processed file, loading data...\")\n",
    "\n",
    "    \n",
    "    # Load the dataset from the output file\n",
    "    cov_data, genre_data = load_dataset(output_file)\n",
    "    print(\"Finished loading data.\")\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    for n_neighbors in range(1, 11):\n",
    "        # Split the dataset into training and testing sets\n",
    "        train_X, test_X, train_y, test_y, = train_test_split(\n",
    "            cov_data, genre_data, test_size=0.2\n",
    "            # , random_state=130\n",
    "            )\n",
    "\n",
    "        \n",
    "        knn = KNeighborsClassifier(\n",
    "            n_neighbors=n_neighbors, \n",
    "            metric=compute_modified_mahalanobis_distance)        \n",
    "\n",
    "        # Train the best model on the training data\n",
    "        knn.fit(train_X, train_y)\n",
    "\n",
    "        print(\"knn fit begins\")\n",
    "        # Evaluate the model on the test data\n",
    "        accuracy = knn.score(test_X, test_y)\n",
    "        print(\"finish evaluating the model on the test data\")\n",
    "\n",
    "\n",
    "        # Print the results\n",
    "        print(f'K Value: {n_neighbors}')\n",
    "        print(f'Test Accuracy: {accuracy}')\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing processed file, loading data...\n",
      "Finished loading data.\n",
      "knn fit begins\n",
      "knn fit ends\n",
      "finish evaluating the model on the test data\n",
      "K Value: 1\n",
      "Test Accuracy: 0.9482470784641068\n",
      "\n",
      "knn fit begins\n",
      "knn fit ends\n",
      "finish evaluating the model on the test data\n",
      "K Value: 2\n",
      "Test Accuracy: 0.8564273789649416\n",
      "\n",
      "knn fit begins\n",
      "knn fit ends\n",
      "finish evaluating the model on the test data\n",
      "K Value: 3\n",
      "Test Accuracy: 0.8464106844741235\n",
      "\n",
      "knn fit begins\n",
      "knn fit ends\n",
      "finish evaluating the model on the test data\n",
      "K Value: 4\n",
      "Test Accuracy: 0.8330550918196995\n",
      "\n",
      "knn fit begins\n",
      "knn fit ends\n",
      "finish evaluating the model on the test data\n",
      "K Value: 5\n",
      "Test Accuracy: 0.8080133555926544\n",
      "\n",
      "knn fit begins\n",
      "knn fit ends\n",
      "finish evaluating the model on the test data\n",
      "K Value: 6\n",
      "Test Accuracy: 0.7813021702838063\n",
      "\n",
      "knn fit begins\n",
      "knn fit ends\n",
      "finish evaluating the model on the test data\n",
      "K Value: 7\n",
      "Test Accuracy: 0.8030050083472454\n",
      "\n",
      "knn fit begins\n",
      "knn fit ends\n",
      "finish evaluating the model on the test data\n",
      "K Value: 8\n",
      "Test Accuracy: 0.8130217028380634\n",
      "\n",
      "knn fit begins\n",
      "knn fit ends\n",
      "finish evaluating the model on the test data\n",
      "K Value: 9\n",
      "Test Accuracy: 0.7362270450751253\n",
      "\n",
      "knn fit begins\n",
      "knn fit ends\n",
      "finish evaluating the model on the test data\n",
      "K Value: 10\n",
      "Test Accuracy: 0.7495826377295493\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
