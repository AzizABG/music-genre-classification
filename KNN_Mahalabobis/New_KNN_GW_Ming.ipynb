{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary libraries\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import scipy.io.wavfile as wav\n",
    "from python_speech_features import mfcc\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_audio_files(directory, output_file):\n",
    "    \"\"\"\n",
    "    Extract MFCC features from audio files in the specified directory \n",
    "    and save them to an output file.\n",
    "    \"\"\"\n",
    "    min_length = float('inf')  # Initialize minimum length to a very high value\n",
    "\n",
    "\n",
    "\n",
    "    # Find the minimum length among all MFCC features\n",
    "    for i, folder in enumerate(filter(lambda f: f != \".DS_Store\", os.listdir(directory)), start=1):\n",
    "        if i == 11: break\n",
    "        \n",
    "        for file in os.listdir(os.path.join(directory, folder)):\n",
    "            if file == \".DS_Store\":\n",
    "                continue\n",
    "\n",
    "            # Read audio file and extract MFCC features\n",
    "            rate, sig = wav.read(os.path.join(directory, folder, file))\n",
    "            mfcc_feat = mfcc(sig, rate, winlen=0.020, appendEnergy=False, nfft=1024)\n",
    "\n",
    "            # Update the minimum length of MFCC features\n",
    "            min_length = min(min_length, mfcc_feat.shape[0])\n",
    "\n",
    "\n",
    "    with open(output_file, 'wb') as f:  # Open file to write the MFCC features\n",
    "        # Iterate over folders in the directory\n",
    "        for i, folder in enumerate(filter(lambda f: f != \".DS_Store\", os.listdir(directory)), start=1):\n",
    "            if i == 11: break\n",
    "            \n",
    "            for file in os.listdir(os.path.join(directory, folder)):\n",
    "                if file == \".DS_Store\":\n",
    "                    continue\n",
    "\n",
    "                # Read audio file and extract MFCC features\n",
    "                rate, sig = wav.read(os.path.join(directory, folder, file))\n",
    "                mfcc_feat = mfcc(sig, rate, winlen=0.020, appendEnergy=False, nfft=1024)\n",
    "                mfcc_feat_truncated = mfcc_feat[:min_length, :]\n",
    "                # Save the MFCC features and corresponding folder label\n",
    "                pickle.dump((mfcc_feat_truncated, i), f)\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gwtau_distance(X_flat, Y_flat, n_features=13):\n",
    "    \"\"\"\n",
    "    Compute the GW distance between two flattened time series with a specified number of features.\n",
    "    \"\"\"\n",
    "    # Reshape flattened arrays back to 2D for distance calculation\n",
    "    X = X_flat.reshape(-1, n_features)\n",
    "    Y = Y_flat.reshape(-1, n_features)\n",
    "\n",
    "    # Calculate the L2 (Euclidean) norm of differences between consecutive points\n",
    "    norm_vec_x = np.linalg.norm(np.diff(X, axis=0), ord=2, axis=1)\n",
    "    norm_vec_y = np.linalg.norm(np.diff(Y, axis=0), ord=2, axis=1)\n",
    "\n",
    "    # Compute cumulative sums of the normalized vectors\n",
    "    v1, v2 = np.cumsum(norm_vec_x), np.cumsum(norm_vec_y)\n",
    "\n",
    "    # Calculate the GWtau distance\n",
    "    return np.linalg.norm(v1 - v2, ord=2) / np.sqrt(len(v1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(filename):\n",
    "    \"\"\"\n",
    "    Load the dataset from a file, trimming each MFCC feature sequence to the minimum length.\n",
    "    \"\"\"\n",
    "    mfcc_data, genre_data = [], []\n",
    "\n",
    "    with open(filename, 'rb') as f:  # Open the file for reading\n",
    "        while True:  # Continue until EOF is reached\n",
    "            try:\n",
    "                # Load the MFCC feature and corresponding genre\n",
    "                mfcc_feat, genre = pickle.load(f)\n",
    "\n",
    "                mfcc_data.append(mfcc_feat)\n",
    "                genre_data.append(genre)\n",
    "\n",
    "            except EOFError:  # Break the loop when EOF is reached\n",
    "                break\n",
    "\n",
    "    # Flatten the MFCC data and convert genre data to NumPy array\n",
    "    return np.vstack([data.flatten() for data in mfcc_data]), np.array(genre_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Use RandomizedSearchCV or GridSearchCV to do hyper parameter tuning\n",
    "\n",
    "\n",
    "# def main():\n",
    "#     # Directory containing the audio files and output file name\n",
    "#     directory = \"./Data/genres_original/\"\n",
    "#     output_file = \"my_mfcc.dat\"\n",
    "\n",
    "#     # Preprocess the audio files to extract and save MFCC features\n",
    "#     if not os.path.exists(output_file):\n",
    "#         # If the file does not exist, preprocess the audio files to extract features\n",
    "#         print(\"Preprocessing audio files and extracting features...\")\n",
    "#         preprocess_audio_files(directory, output_file)\n",
    "#     else:\n",
    "#     # If the file exists, skip preprocessing and use the existing file\n",
    "#     print(\"Found existing processed file, loading data...\")\n",
    "    \n",
    "#     # Load the dataset from the output file\n",
    "#     mfcc_data_flat, genre_data = load_dataset(output_file)\n",
    "    \n",
    "#     # Define the parameters for grid search\n",
    "#     # param_grid = {'n_neighbors': range(1,10), 'metric': [compute_gwtau_distance]}\n",
    "    \n",
    "#     # Define parameters for grid search\n",
    "#     param_grid = {'n_neighbors': range(4,6),\n",
    "#                   'metric': [compute_gwtau_distance]\n",
    "#                   }\n",
    "    \n",
    "#     knn = KNeighborsClassifier()\n",
    "#     grid_search = GridSearchCV(estimator=knn, param_grid=param_grid, cv=5, scoring='accuracy')\n",
    "\n",
    "#     # Perform grid search to find the best KNN parameters\n",
    "#     grid_search.fit(mfcc_data_flat, genre_data)\n",
    "    \n",
    "#     # Get the best parameters and model from the grid search\n",
    "#     best_k = grid_search.best_params_['n_neighbors']\n",
    "#     best_model = grid_search.best_estimator_\n",
    "\n",
    "#     # Split the dataset into training and testing sets\n",
    "#     train_X, test_X, train_y, test_y = train_test_split(mfcc_data_flat, genre_data, test_size=0.2, random_state=130)\n",
    "\n",
    "#     # Train the best model on the training data\n",
    "#     best_model.fit(train_X, train_y)\n",
    "\n",
    "#     # Evaluate the model on the test data\n",
    "#     accuracy = best_model.score(test_X, test_y)\n",
    "\n",
    "#     # Print the results\n",
    "#     print(f'Best K: {best_k}')\n",
    "#     print(f'Test Accuracy: {accuracy}')\n",
    "    \n",
    "#     print(mfcc_data_flat.shape)\n",
    "#     print(genre_data.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do not use RandomizedSearchCV or GridSearchCV\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def main():\n",
    "    # Directory containing the audio files and output file name\n",
    "    directory = \"../datasets/raw_data/GTZAN_Dataset/genres_original/\"\n",
    "    output_file = \"my_mfcc.dat\"\n",
    "\n",
    "    # preprocess_audio_files(directory, output_file)\n",
    "    \n",
    "    \n",
    "    # Preprocess the audio files to extract and save MFCC features\n",
    "    if not os.path.exists(output_file):\n",
    "        # If the file does not exist, preprocess the audio files to extract features\n",
    "        print(\"Preprocessing audio files and extracting features...\")\n",
    "        preprocess_audio_files(directory, output_file)\n",
    "    else:\n",
    "    # If the file exists, skip preprocessing and use the existing file\n",
    "        print(\"Found existing processed file, loading data...\")\n",
    "    \n",
    "    # Load the dataset from the output file\n",
    "    mfcc_data_flat, genre_data = load_dataset(output_file)\n",
    "    \n",
    "    # Define the parameters for grid search\n",
    "    # param_grid = {'n_neighbors': range(1,10), 'metric': [compute_gwtau_distance]}\n",
    "    \n",
    "    for n_neighbors in range(1, 11):\n",
    "        knn = KNeighborsClassifier(\n",
    "            n_neighbors=n_neighbors, metric=compute_gwtau_distance)    \n",
    "    \n",
    "   \n",
    "\n",
    "        # Split the dataset into training and testing sets\n",
    "        train_X, test_X, train_y, test_y = train_test_split(mfcc_data_flat, genre_data, test_size=0.2, random_state=130)\n",
    "\n",
    "        # Train the best model on the training data\n",
    "        knn.fit(train_X, train_y)\n",
    "\n",
    "        # Evaluate the model on the test data\n",
    "        accuracy = knn.score(test_X, test_y)\n",
    "\n",
    "        # Print the results\n",
    "        print(f'K Value: {n_neighbors}')\n",
    "        print(f'Test Accuracy: {accuracy}')\n",
    "        print()\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing audio files and extracting features...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K Value: 1\n",
      "Test Accuracy: 0.25\n",
      "\n",
      "K Value: 2\n",
      "Test Accuracy: 0.225\n",
      "\n",
      "K Value: 3\n",
      "Test Accuracy: 0.255\n",
      "\n",
      "K Value: 4\n",
      "Test Accuracy: 0.245\n",
      "\n",
      "K Value: 5\n",
      "Test Accuracy: 0.235\n",
      "\n",
      "K Value: 6\n",
      "Test Accuracy: 0.22\n",
      "\n",
      "K Value: 7\n",
      "Test Accuracy: 0.21\n",
      "\n",
      "K Value: 8\n",
      "Test Accuracy: 0.19\n",
      "\n",
      "K Value: 9\n",
      "Test Accuracy: 0.18\n",
      "\n",
      "K Value: 10\n",
      "Test Accuracy: 0.195\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
